{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0b01975ddddaf4db0ddd9e77ba558bc384051aec2d64c8688cf2fb07acba12100",
   "display_name": "Python 3.9.2 64-bit ('JUST': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import utils\n",
    "from visualization.simple_data_vis import histograms\n",
    "import surrogate_models.dab_nn_defs as engine\n",
    "import kerastuner as kt\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "source": [
    "# load data from just-private/data\n",
    "filename = 'mol_res_scan_results_7.csv'\n",
    "data = utils.load_data(filename)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,_ = histograms(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since currently data is just one big dataframe, select model inputs as X and purity, yield as Y\n",
    "x = [*data.columns[:2],*data.columns[4:]]\n",
    "y = data.columns[2:4]\n",
    "\n",
    "# split data into train and test\n",
    "train_x, test_x, train_y, test_y = utils.chroma_train_test_split(data, x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x = utils.preprocessing([train_x, test_x], standarize = True, skip = ['cut 1','cut 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# define Probabilistic Bayesian Neural Network\n",
    "prob_bnn_model = engine.create_probablistic_bnn_model(\n",
    "    FEATURE_NAMES = data.columns[4:],\n",
    "    TARGET_NAMES = data.columns[2:4], \n",
    "    train_size = train_size, \n",
    "    n_outputs = y.shape[1],\n",
    "    hidden_units = [16,8,4],\n",
    "    name = 'PBNN_'+filename\n",
    "    )\n",
    "\n",
    "# specify train/test routine \n",
    "engine.run_experiment(\n",
    "    model = prob_bnn_model, \n",
    "    loss = negative_loglikelihood, \n",
    "    learning_rate = 0.05,\n",
    "    num_epochs = 400,\n",
    "    train_dataset = train_dataset, \n",
    "    test_dataset = test_dataset\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "sample_inputs, sample_outputs = list(test_dataset.unbatch().shuffle(dataset_size).batch(n_samples))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    hp_layers = hp.Int('layers', min_value=2, max_value=10)\n",
    "    hp_units = hp.Int('units', min_value=4, max_value = 32, step=2)\n",
    "    for L in range(hp_layers):\n",
    "        hidden_units = [np.ceil(hp_units**(l/L)) for l in range(L)]\n",
    "\n",
    "    # define Probabilistic Bayesian Neural Network \n",
    "    model = engine.create_probablistic_bnn_model(\n",
    "        FEATURE_NAMES = data.columns[4:],\n",
    "        TARGET_NAMES = data.columns[2:4], \n",
    "        train_size = train_size, \n",
    "        n_outputs = y.shape[1],\n",
    "        hidden_units = hidden_units,\n",
    "        name = 'PBNN_'+filename\n",
    "        )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(learning_rate=hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n",
    "        loss=engine.negative_loglikelihood,\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_loss',\n",
    "                     max_epochs=400,\n",
    "                     factor=2,\n",
    "                     directory=\"surrogate_models/.hypertuning/\",\n",
    "                     project_name='intro_to_kt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "tuner.search(train_dataset, validation_data = test_dataset, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = tuner.get_best_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_distributions = best[0](sample_inputs)\n",
    "best_means = [i.mean().numpy().tolist() for i in best_distributions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "test_pbnn_model = engine.create_probablistic_bnn_model(\n",
    "    FEATURE_NAMES = data.columns[4:],\n",
    "    TARGET_NAMES = data.columns[2:4], \n",
    "    train_size = train_size, \n",
    "    n_outputs = y.shape[1],\n",
    "    hidden_units = [512,32],\n",
    "    name = 'test_PBNN_'+filename\n",
    "    )\n",
    "\n",
    "# specify train/test routine \n",
    "engine.run_experiment(\n",
    "    model = test_pbnn_model, \n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    learning_rate = 0.005,\n",
    "    num_epochs = 300,\n",
    "    train_dataset = train_dataset, \n",
    "    test_dataset = test_dataset,\n",
    "    verbose = 1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "N = 10\n",
    "out = []\n",
    "for i in range(N):\n",
    "    out.append(test_pbnn_model(sample_inputs))\n",
    "out = tf.stack(out,-1)\n",
    "\n",
    "colors = ['k','r']\n",
    "for i in range(2):\n",
    "    plt.errorbar(list(sample_outputs.values())[i].numpy(), out[i,:,:,:].numpy().mean(-1),\n",
    "    yerr = out[i,:,:,:].numpy().std(-1).squeeze(),\n",
    "    label = data.columns[2:4][i],\n",
    "    marker = 'o', color = colors[i], alpha = 0.5,\n",
    "    ls = 'none')\n",
    "plt.plot([0,1],[0,1],'k',alpha=0.25)\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.xlabel('true')\n",
    "plt.ylabel('predicted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}